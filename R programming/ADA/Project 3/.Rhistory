print("Hello")
getwd()
chapters <- VCorpus(DirSource("./20000Leagues", ignore.case = TRUE, mode = "text"))
install.packages("tm")
library(tm)
chapters <- VCorpus(DirSource("./20000Leagues", ignore.case = TRUE, mode = "text"))
chapters <- VCorpus(DirSource("./dataset", ignore.case = TRUE, mode = "text"))
inspect(chapters)
str(chapters)
chapters
chapters[[0]]
chapters
inspect(chapters)
chapters <- VCorpus(DirSource("./dataset", ignore.case = TRUE, mode = "text"))
inspect(chapters)
str(chapters)
inspect(chapters)
str(chapters)
chapters
install.packages("tokenizers")
library(tokenizers)
longest_words <- function(ch){
words_doc = c()
w <- tokenizers::tokenize_words(ch$content)
print(w)
# for (i in 1:length(w)) {
#   l <- w[[i]]
#   if(length(l) != 0){
#     for(j in 1:length(l)){
#       words_doc[length(words_doc) + 1] <- l[j]
#     }
#   }
# }
#
# uwords <- unique(words_doc)
# uwords[order(-nchar(uwords), uwords)][1:10]
}
longest_words(chapters[[0]])
print(chapters)
print(chapters[0])
str(chapters[0])
str(chapters)
chapters[[1]]
w <- tokenizers::tokenize_words(chapters[[1]]$content)
w
longest_sentences <- function(doc, limit = 10){
sentences <- tokenizers::tokenize_sentences(paste(doc$content,collapse=" "))[[1]]
sentences[order(-nchar(sentences), sentences)][1:limit]
}
View(longest_sentences)
View(longest_sentences)
w
help(tokenize_words)
rm(list = ls())
dev.off()  # But only if there IS a plot
cat("\014")  # ctrl+L
chapters <- VCorpus(DirSource("./dataset", ignore.case = TRUE, mode = "text"))
